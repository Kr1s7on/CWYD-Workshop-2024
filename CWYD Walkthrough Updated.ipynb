{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAhsUTqCDeGq"
   },
   "source": [
    "<img src=\"https://about.nyp.ai/static/logo/Dark.png\" alt=\"NYP AI Logo\" height=\"100px\">\n",
    "\n",
    "# **Introduction to LangChain: Chat With Your Data**\n",
    "\n",
    "Updated: 10 July 2024, 10:33PM\n",
    "\n",
    "**Welcome to NYP AI's Chat With Your Data Workshop.**\n",
    "\n",
    "**What?**\n",
    "\n",
    "In this workshop, code along with the instructor and build your own data inference algorithm with the [LangChain Python library](https://langchain.com).\n",
    "\n",
    "**HELP MEE**\n",
    "\n",
    "For Non-Technical or Curious Questions:\n",
    "\n",
    "> Ask them [here](https://qna.nyp.ai/ask) and the instructor will try to answer them in inter-segment breaks or at the Q&A session at the end.\n",
    "\n",
    "For Technical Questions or Having Trouble Following Along:\n",
    "> Feel free to raise your hand at any time and one of the workshop troubleshooters will assist you.\n",
    "\n",
    "***Note: Please be polite and co-operative, we want to ensure you have a good learning experience and we hope that you will allow us to create that.***\n",
    "\n",
    "**How do I start?**\n",
    "\n",
    "For instructions on setting up this notebook, look at the [CWYD Workshop Pre-Requisites](https://docs.google.com/document/d/e/2PACX-1vRwOmZCrFxrWwbTamFt9eBxprybm4_xNUaSUofVW3Iys50IM15i9yF9oqjmWd32GuG6ZCqYMIo3XVFl/pub) document.\n",
    "\n",
    "---\n",
    "\n",
    "We hope you have takeaway valuable skills from today and that you had fun! ðŸ¤©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDk6MIQDHzf4"
   },
   "source": [
    "# 0. Setup\n",
    "Let's get started!\n",
    "Start by installing the required libraries and getting your workshop account API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvu8lP0JH8kG"
   },
   "source": [
    "### 0.1 Install required libraries\n",
    "The following libraries and imports are required for this notebook.\n",
    "\n",
    "> **Run** the following cell by clicking on the cell and doing **'Shift + Enter'** or clicking the run button at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ci8E90M6JzSt",
    "outputId": "af17c9f0-ecb3-4545-9dd4-e2d5c85f4da9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.2.7-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.35.13-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.15-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting lark\n",
      "  Downloading lark-1.1.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.31-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.5-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.12 (from langchain)\n",
      "  Downloading langchain_core-0.2.16-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.85-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.0/61.0 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "     ---------------------------------------- 0.0/125.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 125.2/125.2 kB 7.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.12.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting chroma-hnswlib==0.7.5 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.5-cp312-cp312-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.18.1-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.65.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.1.3-cp39-abi3-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-4.1.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.6-cp312-none-win_amd64.whl.metadata (51 kB)\n",
      "     ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 51.6/51.6 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (24.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting jinja2>=2.11.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading ujson-5.10.0-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.32.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.12->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.27.2-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading sympy-1.13.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=7.1,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting setuptools>=16.0 (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached setuptools-70.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.20.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai)\n",
      "  Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.23.3)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.1-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.22.0-cp312-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-12.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.0)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb)\n",
      "  Downloading MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer>=0.9.0->chromadb)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.17.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading langchain-0.2.7-py3-none-any.whl (983 kB)\n",
      "   ---------------------------------------- 0.0/983.6 kB ? eta -:--:--\n",
      "   ----------------------------- --------- 737.3/983.6 kB 23.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 983.6/983.6 kB 20.7 MB/s eta 0:00:00\n",
      "Downloading openai-1.35.13-py3-none-any.whl (328 kB)\n",
      "   ---------------------------------------- 0.0/328.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 328.5/328.5 kB 19.9 MB/s eta 0:00:00\n",
      "Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.4/2.2 MB 30.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 23.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 23.7 MB/s eta 0:00:00\n",
      "Downloading langchain_openai-0.1.15-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.1/46.1 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading chromadb-0.5.4-py3-none-any.whl (581 kB)\n",
      "   ---------------------------------------- 0.0/581.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 581.4/581.4 kB 17.9 MB/s eta 0:00:00\n",
      "Downloading chroma_hnswlib-0.7.5-cp312-cp312-win_amd64.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/152.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.0/152.0 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
      "   ---------------------------------------- 0.0/111.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 111.7/111.7 kB 6.3 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp312-cp312-win_amd64.whl (369 kB)\n",
      "   ---------------------------------------- 0.0/369.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 369.0/369.0 kB ? eta 0:00:00\n",
      "Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.8/86.8 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading bcrypt-4.1.3-cp39-abi3-win_amd64.whl (158 kB)\n",
      "   ---------------------------------------- 0.0/158.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 158.1/158.1 kB ? eta 0:00:00\n",
      "Downloading build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "   ---------------------------------------- 0.0/92.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 92.0/92.0 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.65.0-cp312-cp312-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 2.0/4.1 MB 42.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.1/4.1 MB 52.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.1/4.1 MB 43.8 MB/s eta 0:00:00\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.6/75.6 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.9/77.9 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.7/1.7 MB 112.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 54.7 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.2.16-py3-none-any.whl (362 kB)\n",
      "   ---------------------------------------- 0.0/362.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 362.4/362.4 kB ? eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.85-py3-none-any.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 127.9/127.9 kB ? eta 0:00:00\n",
      "Downloading mmh3-4.1.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.2/15.5 MB 45.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 4.4/15.5 MB 46.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 6.6/15.5 MB 42.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.5/15.5 MB 41.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.5/15.5 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.5/15.5 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.5/15.5 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 34.4 MB/s eta 0:00:00\n",
      "Downloading onnxruntime-1.18.1-cp312-cp312-win_amd64.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.9/5.6 MB 39.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.7/5.6 MB 39.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.6/5.6 MB 39.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 35.7 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
      "   ---------------------------------------- 0.0/59.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 59.9/59.9 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 52.5/52.5 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
      "   ---------------------------------------- 0.0/130.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 130.5/130.5 kB 8.0 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 107.0/107.0 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading orjson-3.10.6-cp312-none-win_amd64.whl (136 kB)\n",
      "   ---------------------------------------- 0.0/136.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 136.4/136.4 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.3/41.3 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "   ---------------------------------------- 0.0/423.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 423.9/423.9 kB 25.9 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.20.1-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.8/1.9 MB 37.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 41.0 MB/s eta 0:00:00\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading SQLAlchemy-2.0.31-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.6/2.1 MB 34.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 33.4 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.7.0-cp312-cp312-win_amd64.whl (799 kB)\n",
      "   ---------------------------------------- 0.0/799.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 799.3/799.3 kB 24.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.2/2.2 MB 46.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 46.9 MB/s eta 0:00:00\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.2/47.2 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.4/62.4 kB ? eta 0:00:00\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB ? eta 0:00:00\n",
      "Downloading google_auth-2.32.0-py2.py3-none-any.whl (195 kB)\n",
      "   ---------------------------------------- 0.0/195.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 195.5/195.5 kB 12.4 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/220.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 220.0/220.0 kB 13.1 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp312-cp312-win_amd64.whl (293 kB)\n",
      "   ---------------------------------------- 0.0/293.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 293.6/293.6 kB 17.7 MB/s eta 0:00:00\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading httptools-0.6.1-cp312-cp312-win_amd64.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.7/55.7 kB ? eta 0:00:00\n",
      "Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.3/133.3 kB ? eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.2/49.2 kB ? eta 0:00:00\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading multidict-6.0.5-cp312-cp312-win_amd64.whl (27 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.7/151.7 kB ? eta 0:00:00\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 25.2 MB/s eta 0:00:00\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 268.5/268.5 kB ? eta 0:00:00\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 240.7/240.7 kB 14.4 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading ujson-5.10.0-cp312-cp312-win_amd64.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.2/42.2 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading watchfiles-0.22.0-cp312-none-win_amd64.whl (280 kB)\n",
      "   ---------------------------------------- 0.0/281.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 281.0/281.0 kB ? eta 0:00:00\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading websockets-12.0-cp312-cp312-win_amd64.whl (124 kB)\n",
      "   ---------------------------------------- 0.0/125.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 125.0/125.0 kB 7.2 MB/s eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB ? eta 0:00:00\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.13.0-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.5/6.2 MB 32.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.6/6.2 MB 38.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.4/6.2 MB 38.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 35.9 MB/s eta 0:00:00\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 307.7/307.7 kB ? eta 0:00:00\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n",
      "Downloading MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl (17 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 32.9 MB/s eta 0:00:00\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.2/181.2 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached setuptools-70.3.0-py3-none-any.whl (931 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Downloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 85.3/85.3 kB ? eta 0:00:00\n",
      "Using cached pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53836 sha256=3c826d7ccf56b6ab40d4a028cbfe5501faf34aaf5912eb6d91e06d5ab88c41c0\n",
      "  Stored in directory: c:\\users\\werty\\appdata\\local\\pip\\cache\\wheels\\d5\\3d\\69\\8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pyreadline3, pypika, mpmath, monotonic, mmh3, flatbuffers, zipp, wrapt, websockets, websocket-client, ujson, tenacity, sympy, sniffio, shellingham, setuptools, regex, python-multipart, python-dotenv, pyproject_hooks, pydantic-core, pyasn1, protobuf, overrides, orjson, opentelemetry-util-http, oauthlib, numpy, mypy-extensions, multidict, mdurl, marshmallow, MarkupSafe, lark, jsonpointer, importlib-resources, humanfriendly, httptools, h11, grpcio, greenlet, frozenlist, dnspython, distro, click, cachetools, bcrypt, backoff, attrs, asgiref, annotated-types, yarl, uvicorn, typing-inspect, tiktoken, SQLAlchemy, rsa, requests-oauthlib, pydantic, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, jsonpatch, jinja2, importlib-metadata, httpcore, googleapis-common-protos, email_validator, deprecated, coloredlogs, chroma-hnswlib, build, anyio, aiosignal, watchfiles, tokenizers, starlette, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, langsmith, httpx, google-auth, dataclasses-json, aiohttp, typer, opentelemetry-semantic-conventions, opentelemetry-instrumentation, openai, langchain-core, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation-asgi, langchain-text-splitters, langchain-openai, fastapi-cli, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain, fastapi, langchain-community, chromadb\n",
      "Successfully installed MarkupSafe-2.1.5 SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.4.0 asgiref-3.8.1 attrs-23.2.0 backoff-2.2.1 bcrypt-4.1.3 build-1.2.1 cachetools-5.3.3 chroma-hnswlib-0.7.5 chromadb-0.5.4 click-8.1.7 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 distro-1.9.0 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 flatbuffers-24.3.25 frozenlist-1.4.1 google-auth-2.32.0 googleapis-common-protos-1.63.2 greenlet-3.0.3 grpcio-1.65.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 importlib-metadata-7.1.0 importlib-resources-6.4.0 jinja2-3.1.4 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-30.1.0 langchain-0.2.7 langchain-community-0.2.7 langchain-core-0.2.16 langchain-openai-0.1.15 langchain-text-splitters-0.2.2 langsmith-0.1.85 lark-1.1.9 markdown-it-py-3.0.0 marshmallow-3.21.3 mdurl-0.1.2 mmh3-4.1.0 monotonic-1.6 mpmath-1.3.0 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 oauthlib-3.2.2 onnxruntime-1.18.1 openai-1.35.13 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-grpc-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 orjson-3.10.6 overrides-7.7.0 posthog-3.5.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.8.2 pydantic-core-2.20.1 pypika-0.48.9 pyproject_hooks-1.1.0 pyreadline3-3.4.1 python-dotenv-1.0.1 python-multipart-0.0.9 regex-2024.5.15 requests-oauthlib-2.0.0 rich-13.7.1 rsa-4.9 setuptools-70.3.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.37.2 sympy-1.13.0 tenacity-8.5.0 tiktoken-0.7.0 tokenizers-0.19.1 typer-0.12.3 typing-inspect-0.9.0 ujson-5.10.0 uvicorn-0.30.1 watchfiles-0.22.0 websocket-client-1.8.0 websockets-12.0 wrapt-1.16.0 yarl-1.9.4 zipp-3.19.2\n",
      "\n",
      "All libraries and imports successful!\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai langchain-community langchain-openai chromadb lark\n",
    "import os, sys, json, shutil\n",
    "from pprint import pprint\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "print()\n",
    "print(\"All libraries and imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbnVMBSFLo8O"
   },
   "source": [
    "### 0.2 Get the workshop API Key\n",
    "\n",
    "This API key will be used by LangChain to send structured queries to OpenAI endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPEce6J6LuL0"
   },
   "outputs": [],
   "source": [
    "# Run this cell first\n",
    "exec(\"\"\"\\nimport os, sys, json, requests\\n\\ndef injectAPIKey(username,password,injectionKey=\"OPENAI_API_KEY\"):\\n    hd = {\"Content-Type\":\"application/json\",\"APIKey\":\"P@ssw0rd!\"}\\n    d = requests.post(url=\"https://keyserver.replit.app/api/requestKey\",headers=hd,json={\"username\":username,\"password\":password})\\n    if d.text.startswith(\"UERROR\") or d.text.startswith(\"ERROR\"):\\n        raise Exception(\"INJECTAPIKEY ERROR: \" + d.text[len(\"ERROR: \"):])\\n    elif d.text.startswith(\"SUCCESS\"):\\n        os.environ[injectionKey] = d.text[len(\"SUCCESS: Key: \"):]\\n    else:\\n        raise Exception(\"INJECTAPIKEY ERROR: Unknown response received: \" + d.text)\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng2rkwWoljh4"
   },
   "source": [
    "**Where to get the USERNAME and PASSWORD?**\n",
    "\n",
    "Details should've been sent to your email. Pass it into the `injectAPIKey` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac1bgcOaJGWu"
   },
   "outputs": [],
   "source": [
    "# Uncomment the line of code below and replace parameters with your username and password\n",
    "injectAPIKey(\"231165R\", \"30g1on7w\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3hWK-pVJINl"
   },
   "source": [
    "---\n",
    "\n",
    "# Segment 1 - Loading, Splitting, Embedding\n",
    "Now that you have finished setting up, let's get right into it!\n",
    "\n",
    "Here, you'll be learning the theory of Retrieval Augmented Generation and the different stages in the process.\n",
    "\n",
    "In the hour, we will tough on Document Loaders, Splitters and Embedding splits into a Vector Database; all part of the **Indexing Pipline** shown below.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vAvDBIbr8MnL_Q51mBtBhw.png\" alt=\"Indexing Pipeline\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xydUKvSpJLoS"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What are LLMs?**\n",
    "\n",
    "Large Language Models (LLMs) demonstrate significant capabilities in understanding and generating human language. They can perform various tasks such as answering questions, summarizing text, and generating creative content.\n",
    "\n",
    "However, they **sometimes generate incorrect but believable responses** when they lack information, a **phenomenon known as â€œhallucination.â€** This means they **confidently provide information that may sound accurate but could be incorrect due to outdated or insufficient knowledge**.\n",
    "\n",
    "> In the context of this workshop, LLMs are powerful tools, but they need proper mechanisms to ensure the accuracy and relevance of their responses.\n",
    "\n",
    "## Where RAG comes in...\n",
    "\n",
    "**What is RAG?**\n",
    "\n",
    "Retrieval Augmented Generation (RAG) **addresses the issue of LLM hallucinations** by integrating an information retrieval system into the LLM pipeline. Instead of relying solely on pre-trained knowledge, RAG allows the model to dynamically **fetch information from external knowledge sources when generating responses**. This dynamic retrieval mechanism ensures that the information provided by the LLM is not only **contextually relevant** but also **accurate and up-to-date**.\n",
    "\n",
    "> In summary, RAG enhances the reliability of the conversation by grounding responses in real-time data, making interactions more trustworthy and informative.\n",
    "\n",
    "***Below is a simplified RAG pipeline:***\n",
    "\n",
    "<img src=\"https://assets-global.website-files.com/5ee50f2ef83ac07f0cb7fb44/65847f3073978e597886d087_rag-f517f1f834bdbb94a87765e0edd40ff2.png\" alt=\"RAG Pipeline\" height=\"400px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGg21djaKdXA"
   },
   "source": [
    "## 1.1 Document Loading\n",
    "\n",
    "**What is Document Loading?**\n",
    "\n",
    "The first step is to transform your data into a format conducive for interaction. We do this by embedding your your source data into a semantic numerical format for retrieval, as you'll learn soon.\n",
    "\n",
    "**Why is it Important?**\n",
    "\n",
    "Document loaders play a crucial role in ***accessing and converting data from a multitude of formats and sources into a standardized structure.***\n",
    "\n",
    "We often find ourselves needing to extract data from various origins such as websites, databases, YouTube, and this data can manifest in diverse formats like PDFs, HTML, and JSON. The primary objective of document loaders is to harmonize this data diversity into a unified document object, comprising content and associated metadata.\n",
    "\n",
    "**Where Langchain Comes In...**\n",
    "\n",
    "In LangChain, you'll discover an extensive range of ***document loaders***, roughly categorized into more than 80 distinct types. These loaders cater to unstructured data, such as text files from public sources like YouTube, Twitter, or Hacker News, as well as unstructured data from proprietary sources like Figma or Notion.\n",
    "\n",
    "> Document loaders also extend their capabilities to structured data, often presented in tabular formats, containing text data within cells or rows that are still essential for question answering or semantic search.\n",
    "\n",
    "For this workshop, we will be using a Notion Database of Harry Potter information.\n",
    "\n",
    "> **How to Load Notion Databases (IN GENERAL):**\n",
    ">\n",
    "> 1. Export your Notion space as Markdown/CSV\n",
    "> 2. Enable 'Include subpages' and 'Create folders for subspages'\n",
    "> 3. Unzip the folder and place it in the same folder as this .ipynb file\n",
    "> 4. Use Langchain's Document Loader to load your Notion DB with steps similar to what's shown.\n",
    "\n",
    "Follow the instructions below to setup the Harry Potter Notion DB and load it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhkC5DclJzUB"
   },
   "source": [
    "### Prepare Harry Potter Notion DB\n",
    "\n",
    "> In the same folder as this notebook, [download and unzip the HogwartsDB Notion dump](https://go.nyp.ai/cwydhogwarts).\n",
    "\n",
    "The unzipped folder should directly have 6 files of Harry Potter text; there should be no sub-folders. Some operating systems may auto-create subfolders with the same name in the unzipped folder, so you need to move the files up one folder.\n",
    "\n",
    "Resulting folder structure should look like:\n",
    "```\n",
    "- CWYD Walkthrough.ipynb\n",
    "- HogwartsDB\n",
    "    - Harry Potter and The Sorcerer's Stone.md\n",
    "    - Harry Potter and the Chamber of Secrets.md\n",
    "    - Harry Potter and the Prisoner of Azkaban.md\n",
    "    - Harry Potter and the Goblet of Fire.md\n",
    "    - Harry Potter and the Order of the Phoenix.md\n",
    "    - Harry Potter and the Half-Blood Prince.md\n",
    "```\n",
    "\n",
    "*If you face issues with unzipping and loading the HogwartsDB in the subsequent steps, seek help from a troubleshooter.*\n",
    "\n",
    "> You need to take note of where you're storing this notebook file. If you don't remember, run the cell below to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7dHRYl4_uBH"
   },
   "outputs": [],
   "source": [
    "# (OPTIONAL) RUN TO CHECK CURRENT FOLDER AND TO SEE IF HOGWARTSDB FOLDER IS FOUND\n",
    "print(\"Notebook's Current Folder Path:\", os.getcwd())\n",
    "print(\"HogwartsDB folder found in current folder:\", os.path.isdir(os.path.join(os.getcwd(), \"HogwartsDB\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEZeHeYa_uBH"
   },
   "source": [
    "### Loading HogwartsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2OT5kMKJ3yz"
   },
   "outputs": [],
   "source": [
    "# Initialise a NotionDirectoryLoader and load the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMIVqHM5NJOU"
   },
   "outputs": [],
   "source": [
    "# Check if data has loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ma3i2r5PO-Oh"
   },
   "outputs": [],
   "source": [
    "# see the metadata of the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neU74-QaWy_R"
   },
   "source": [
    "## 1.2 Text / Document Splitting\n",
    "\n",
    "**What is Document Splitting?**\n",
    "\n",
    "Document splitting is a pre-processing step that ***divides large textual documents into smaller segments or chunks***. This technique is essential for ***managing and processing large volumes of text efficiently***, especially in natural language processing (NLP) tasks.\n",
    "\n",
    "Once documents are split, each segment or chunk becomes more manageable for further analysis and processing. This segmentation allows NLP models to handle pieces of text individually, improving computational efficiency and enabling more targeted analysis.\n",
    "\n",
    "**Chunking Method using Fixed Chunk Sizes & Overlapping**\n",
    "\n",
    "One method to document splitting is by...\n",
    "\n",
    "**Chunk Size**\n",
    "\n",
    "The size of the chunked data is going to make a huge difference in what information comes up in a search. When you embed a piece of data, the whole thing is converted into a vector. Include too much in a chunk and the vector loses the ability to be specific to anything it discusses. Include too little and you lose the context of the data.\n",
    "\n",
    "> In short, size matters. ðŸ˜\n",
    "\n",
    "**Chunk Overlapping**\n",
    "\n",
    "For some LangChain splitters, you can specify a specific chunk overlap; chunk overlaps help to precede chunks with information from the previous chunk so that the chunk split is not too abrupt. The specified quantity of overlap is included in both the end and the beginning of the first and second chunks respectively.\n",
    "\n",
    "This helps chunks be more useful and not too abrupt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfQa_j7nXC-k"
   },
   "source": [
    "### Common LangChain Text Splitters\n",
    "\n",
    "LangChain provides an extensive range of different text splitters. Some common ones include:\n",
    "- Character Text Splitter\n",
    "- Token Text Splitter\n",
    "- Recursive Character Text Splitter\n",
    "- Markdown Header Text Splitter (also known as 'Context-aware chunking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc0f8Bn7_uBI"
   },
   "source": [
    "### Understanding LangChain Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaKR-fDZ_uBI"
   },
   "outputs": [],
   "source": [
    "# Initialise a CharacterTextSplitter and RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep99tN1d_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text1 with r_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtPPO_D7_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text2 with r_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBBVk0bH_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text3 with r_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJA28bHN_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text3 with c_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIkyFCQs_uBI"
   },
   "outputs": [],
   "source": [
    "# Split text3 with new c_splitter with space separator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7QT1sUsSJAj"
   },
   "source": [
    "### Splitting HogwartsDB (RecursiveSplitter)\n",
    "\n",
    "For this workshop, we will be using `RecursiveCharacterTextSplitter` to split our data. As you'll learn, the splitter splits based on a list of separators, ordered by priority in terms of highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV0aSISjW3fO"
   },
   "outputs": [],
   "source": [
    "# Define reasonable chunk parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U-E7QAA_uBI"
   },
   "outputs": [],
   "source": [
    "# Initialise new RecursiveCharacterTextSplitter and split with split_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr9IpGVb_uBI"
   },
   "source": [
    "## 1.3 Embedding Chunks into a Vector Store / Database\n",
    "\n",
    "**What are Embeddings?**\n",
    "\n",
    "Embeddings are numerical representations of real-world objects that machine learning (ML) and AI systems use to understand complex knowledge domains like humans do.\n",
    "\n",
    "As an example, computing algorithms understand that the difference between 2 and 3 is 1, indicating a close relationship between 2 and 3 as compared to 2 and 100.\n",
    "\n",
    "We will be using the `OpenAIEmbeddings` module, which uses embedding models made by OpenAI.\n",
    "\n",
    "**What are Vectorstores?**\n",
    "\n",
    "A vector store is an actual system or platform to handle the complexities and specifics of vector data, like embeddings, often in association with a vector database. They are very commonly used in AI and ML applications.\n",
    "\n",
    "Popular examples of vector databases include Pinecone, Chroma and many more.\n",
    "\n",
    "**Importance of Vectorstores for Embeddings**\n",
    "\n",
    "By storing embeddings in a vector store, we can perform really efficient searches and retrievals, allowing us to retrieve the most relevant documents or chunks of text for a given query.\n",
    "\n",
    "There are many vectorstores that you can use to store your embeddings. For this workshop, we will be using ChromaDB to store our Hogwarts Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTy1mirK_uBI"
   },
   "source": [
    "### Embedding our DB Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrvNwQhT_uBJ"
   },
   "outputs": [],
   "source": [
    "# Initialise OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNv0fXTI_uBJ"
   },
   "outputs": [],
   "source": [
    "# Initialise a Chroma vector database. Persist in a './db/chroma' folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-pEy368_uBJ"
   },
   "outputs": [],
   "source": [
    "# Check vectorDB collection count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLrN1P2t_uBJ"
   },
   "source": [
    "## 1.4 Segment 1 Checkpoint\n",
    "\n",
    "Wow, that was intense! Let's **summarise** what we have learnt so far:\n",
    "1. **Document Loading**\n",
    "  - Using a few of LangChain's loaders to load a Notion Dump\n",
    "\n",
    "2. **Document Splitting**\n",
    "  - Using LangChain's different splitters to split different kinds of data in different ways\n",
    "\n",
    "3. **Embeddings**\n",
    "  - Using the `OpenAIEmbeddings` module (and the `Text-embedding-ada-002-v2` model) to embed splits\n",
    "\n",
    "4. **Vector Storing**\n",
    "  - Storing embeddings into a local vector `Chroma` database\n",
    "\n",
    "\n",
    "While we have only went through the basics, we do encourge you guys to **stay curious** and explore more on the different methods for each step!\n",
    "\n",
    "Explore:\n",
    "- [All the different LangChain loaders available](https://python.langchain.com/v0.2/docs/integrations/document_loaders/)\n",
    "- [Explore different data splitters and parameters](https://python.langchain.com/v0.2/docs/integrations/document_transformers/)\n",
    "- [Learn about how embedding models work](https://medium.com/@eugenesh4work/what-are-embeddings-and-how-do-it-work-b35af573b59e)\n",
    "- [Learn about different Vector stores](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)\n",
    "- [Learn more about Chroma](https://www.trychroma.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dbOU9qH_uBJ"
   },
   "source": [
    "---\n",
    "# Segment 2 - Retrieval Algorithms\n",
    "\n",
    "**What is Retrieval?**\n",
    "\n",
    "After storing our embeddings into a vector store, we can must now look at how we can retrieve the appropriate splits that is relevant to our Prompt / Search Query to load into the LLM.\n",
    "\n",
    "**Importance of Retrieval Algorithms**\n",
    "\n",
    "Retrieval algorithms are then important since they are the core techniques for the retrieval of data in response to a user's query. They are responsible for retrieving information that is potentially useful for the LLM to answer the user appropriately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0KfLMjq_uBK"
   },
   "source": [
    "## 2.1 Common Retrieval Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8TONHnH_uBK"
   },
   "source": [
    "### Semantic Similarity Search\n",
    "\n",
    "**How it works?**\n",
    "\n",
    "Taking advantage of a vector database's properties, this technique allows you to retrieve the most similar document chunks for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm-ed18k_uBK"
   },
   "outputs": [],
   "source": [
    "# Carry out a basic semantic similarity search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaV0UdSc_uBK"
   },
   "source": [
    "### Filtered Similarity Search\n",
    "\n",
    "Building on the basic semantic similarity search, we can add in a filter to it.\n",
    "This `filter` parameter limits the search to ONLY retrieve from the splits inside the stated document souce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_T3Vce0Y_uBK"
   },
   "outputs": [],
   "source": [
    "# Filtered Similarity Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMn5abFG_uBK"
   },
   "source": [
    "### MMR Search (Diverse retrieval)\n",
    "\n",
    "**How it works?**\n",
    "\n",
    "The idea behind Maximum Marginal Relevance (MMR) is to reduce redundancy and increase diversity in the results. MMR selects the phrase in the final keyphrases list according to a combined criterion of query relevance and novelty of information.\n",
    "\n",
    "In LangChain, you provide a initial `fetch_k` to indicate the number of similar chunks you want to retrieve. From this, the specified `k` **diverse** chunks will be returned as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbX_dvMi_uBK"
   },
   "outputs": [],
   "source": [
    "# Maximum Marginal Relevance Search (Diverse retrieval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZX9WcKa_uBK"
   },
   "source": [
    "## 2.2 BONUS: Self-query Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAuB0q5H_uBL"
   },
   "source": [
    "Often, you want to infer from the metadata itself.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes.\n",
    "\n",
    "[Try out self-query retrieval by referring to this.](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWLbBcP9_uBL"
   },
   "source": [
    "## 2.3 Segment 2 Checkpoint\n",
    "\n",
    "This is a good point to stop and explore for a second. Take a review of all that you've learned in this section. Try out different kinds of queries and see the outputs you get. Play around with the parameters you pass in and see what parameters work the best.\n",
    "\n",
    "\n",
    "You can also explore other kinds of search like `asimilarity_search`, `similarity_search_with_score` and many more. Try passing different parameters to the retrieval chains and experimenting with different prompts.\n",
    "\n",
    "[Learn more about the large variety and complexities of LangChain retrievers here](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDomJTTm_uBL"
   },
   "source": [
    "---\n",
    "# Segment 3 - Question Answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOS5QdP9_uBL"
   },
   "source": [
    "## 3.0 Setting Up LangSmith (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8l7x9aE_uBL"
   },
   "source": [
    "This is entirely optional; the instructor will show you the LangSmith console during the workshop to explain what's going on.\n",
    "\n",
    "The benefit of linking up to the LangSmith platform is the ability to visualise the LLM calls and different steps a chain takes.\n",
    "\n",
    "If you want to link up with LangSmith, carry out the following:\n",
    "- Go to [LangSmith](https://www.langchain.com/langsmith) and sign up\n",
    "- Create an API key from your account settings\n",
    "- Uncomment the code below and use your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTYuS0sj_uBL"
   },
   "outputs": [],
   "source": [
    "# Set up LangSmith\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"...\" # replace dots with your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2kh7lBl7nE7"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What is Retrieval Questions Answering Chains?**\n",
    "\n",
    "Retrieval QA chains are designed for question-answeing tasks where the answer is retrieved from a given context. Chains are highly modular; you can combine them with other chains, re-order them and even introduce your own steps in between.\n",
    "\n",
    "**Importance of 'Chains'**\n",
    "\n",
    "Retrieval chains play an important role in the retrieval process, providing a streamlined process of flow and maintaining the efficiency and relevancy of information extracted from external sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O47i0s2x_uBL"
   },
   "source": [
    "## 3.1 Stuff QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAU7IvZY_uBL"
   },
   "source": [
    "### Making Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FjpCdrD_uBL"
   },
   "outputs": [],
   "source": [
    "# Initialise a PromptTemplate with a given string template\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # try experimenting temperature with values from 0-1\n",
    "\n",
    "# Build your prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYV13umD_uBL"
   },
   "source": [
    "### Running a QA Chain (Stuff Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kCMgDz9_uBL"
   },
   "outputs": [],
   "source": [
    "# Initialise a RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMb4V2G8_uBM"
   },
   "outputs": [],
   "source": [
    "# Run user query through the chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-SYJrPi_uBM"
   },
   "outputs": [],
   "source": [
    "# See the result's source documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6XAm0IP_uBM"
   },
   "source": [
    "The stuff technique is really good because it involves only one call to the language model.\n",
    "\n",
    "The problem with this is that if there's too many documents, they may not all be able to fit in the LLM's context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUEkKyVe_uBM"
   },
   "source": [
    "## 3.2 MapReduce QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRe9Yx5s_uBM"
   },
   "source": [
    "In the Map Reduce technique, each retrieved chunk is passed into individual LLM calls to be summarised.\n",
    "\n",
    "These summarised chunks are then stuffed into one final LLM call with the user's prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuusGKF6_uBM"
   },
   "source": [
    "### Create a MapReduce chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A57qsRrF_uBM"
   },
   "outputs": [],
   "source": [
    "# Initialise a MapReduce RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1R3cK3OU_uBM"
   },
   "outputs": [],
   "source": [
    "# Run user query through the MapReduce chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDCB2-23_uBM"
   },
   "source": [
    "> Note how the map reduce chain took **significantly longer**? In some cases, map reduce even **performs worse than a stuff technique**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-ZyhbTf_uBM"
   },
   "source": [
    "### Why is it taking longer?\n",
    "\n",
    "This is due to a few reasons that you can uncover by looking at the run trace in LangSmith:\n",
    "- MapReduce summarises each retrieved chunk in separate LLM calls first\n",
    "- These summarised chunks are then stuffed into a regular `StuffDocumentsChain` with a call to the LLM with the initial user query.\n",
    "- However, **these summarised chunks may not be an accurate representation or may have missing information from the original chunk**, explaining the longer wait times and the inaccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_TMAn7O_uBM"
   },
   "source": [
    "## 3.3 Refine QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIjVbWY0_uBM"
   },
   "source": [
    "In a chain using the refine technique, LangChain will invoke sequential calls to the LLM.\n",
    "\n",
    "In each call, LangChain provides a chunk or more of context to the LLM and prompts with the user question. In subsequent calls, the previous response is **combined with new data/chunks and the LLM is prompted to refine it's original answer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Bt-6-B4_uBM"
   },
   "source": [
    "### Create a Refine chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbspemzD_uBM"
   },
   "outputs": [],
   "source": [
    "# Initialise a Refine RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCyxjGZH_uBN"
   },
   "outputs": [],
   "source": [
    "# Run user query through the Refine chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxqcwLPl_uBN"
   },
   "source": [
    "As you can see, through iterative refinements, the LLM's output is much more well-phrased and comprehensive.\n",
    "\n",
    "The output is also better than when you ran the map reduce chain, because the refine chain actually emphasises more carrying over of information than the former chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KRMOStX_uBN"
   },
   "source": [
    "But, you still can't ask follow up questions. The whole point of a chatbot is to be able to have follow-up questions right?\n",
    "\n",
    "**Let's fix that.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bv3gK6v_uBN"
   },
   "source": [
    "## 3.4 Conversational Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM1SoaOO_uBN"
   },
   "source": [
    "INTRODUCE A BIT OF THEORY ABOUT CONVERSATIONS, AND HOW THEY USE MEMORY BUFFERS. INTRODUCE LANGCHAIN'S CONVERSATIONALRETRIEVALCHAIN.\n",
    "\n",
    "**What are RAG Conversations?**\n",
    "\n",
    "Thanks to the modular architecture of chains, you can nest chains within each other and pass data to and fro.\n",
    "\n",
    "As a result, you can then create conversational chains, so that, while simultaneously retrieving the most relevant information, information about the conversation history is also included to make the answer well-informed.\n",
    "\n",
    "**What are Memory Buffers?**\n",
    "\n",
    "Memory buffers in LangChain allow for the storing of messages which are later formatted into input variables for the prompt. This tool allows you to quickly maintain conversation state and create powerful conversational chains.\n",
    "\n",
    "**What is a `ConversationalRetrievalChain`?**\n",
    "\n",
    "`ConversationalRetrievalChain` is a module from LangChain which allows you to quick create a conversational interface with your data, provided a memory buffer and vector database to retrieve from. This is how we will create a conversational chat interface to talk to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVr26Q8e_uBN"
   },
   "source": [
    "### Create a Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVPWSVpa_uBN"
   },
   "outputs": [],
   "source": [
    "# Initialise a ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XclV0mSd_uBN"
   },
   "source": [
    "### Create a ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSuK82uZ_uBN"
   },
   "outputs": [],
   "source": [
    "# Initialise a ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNsBERHU_uBN"
   },
   "outputs": [],
   "source": [
    "# Run sequential user queries through the chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwrlhUq9_uBN"
   },
   "source": [
    "***And that's it!***\n",
    "\n",
    "**Congratulations! You can now *Chat With Your Data!*** ðŸ¤¯ðŸŽ‰ðŸ¥³"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNSEXTxD_uBN"
   },
   "source": [
    "## 3.5 Segment 3 Checkpoint\n",
    "\n",
    "Excitingly, now you have finally created a working chat interface with your own custom data.\n",
    "\n",
    "Now that you've written the algorithm, hopefully you can see how it all falls into place together to **create a streamlined Retrieval Augmented Generation workflow.**\n",
    "\n",
    "This workflow algorithm is highly modular, you can substitute, modify, add, remove any components or logic however you want, as long as the core concepts and procedures of RAG are there. You can introduce your own custom logic as well for more niche use cases.\n",
    "\n",
    "**As a recap of this segment, you:**\n",
    "- Created a stuff `RetrievalQA`, where you discovered that it may not be ideal for cases where the documents overflow the LLM's context window\n",
    "- Created a map reduce `RetrievalQA`, which summarises chunks (\"reduces\") and then collates them into one final LLM call. But, map reduce is often inaccurate\n",
    "- Created a refine `RetrievalQA`, which incrementally refines an LLM's outputs by combining new data/chunks with previous answers to the prompt\n",
    "\n",
    "\n",
    "**Cheatsheet:**\n",
    "- Fastest - Stuff QA\n",
    "- Slowest - MapReduce QA, Refine QA\n",
    "- Most Accurate & Comprehensive - Refine QA\n",
    "- Least LLM calls - Stuff QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKFF1BEG_uBN"
   },
   "source": [
    "---\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47twLLj1_uBO"
   },
   "source": [
    "**Give yourself a pat on the back for successfully following through this workshop and creating your own data-inferring chatbot! This is no small feat! ðŸŽ‰**\n",
    "\n",
    "\n",
    "**Let's recap all that you have learnt:**\n",
    "- Loading structured and unstructured data with LangChain loaders, especially `NotionDirectoryLoader`\n",
    "- Splitting data in different ways with `RecursiveCharacterTextSplitter` and `CharacterTextSplitter`\n",
    "- Embedding data with `OpenAIEmbeddings` in a local `chroma` vector database\n",
    "- Implementing basic retrieval algorithms like `similarity_search` and `max_marginal_relevance`\n",
    "- Answering questions with chains using `stuff`, `map_reduce` and `refine` techniques\n",
    "- Creating a `ConversationalRetrievalChain` where you can ask follow-up prompts\n",
    "\n",
    "\n",
    "**So, What's next?**\n",
    "\n",
    "You've just learnt the basics of Retrieval Augmented Generation with LangChain in Python. You are now fully equipped to integrate these RAG algorithms into your own personal/school projects for an amazing new AI-powered touchpoint with your users.\n",
    "\n",
    "Additionally, empowered by the basic knowledge, you can go on to further research RAG and all the complex upgrades you introduce in your own algorithms. The world is full of possibilities; **go crazy!**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **CREDITS**\n",
    "\n",
    "**Proudly delivered by...**\n",
    "\n",
    "> This workshop was a massive undertaking in the works for **more than three months** of efforts various people in the **NYP AI Student Interest Group**.\n",
    "\n",
    "\n",
    "At NYP AI internally, we aimed to train ourselves in RAG and developed our own project with team members consisting of:\n",
    "- [Prakhar Nilesh Trivedi](https://linkedin.com/in/prakhartrivedi0706)\n",
    "- [Sarah Zoe Sung](https://www.linkedin.com/in/sarah-zoe-sung/)\n",
    "- [Derron Foo Xi Wei](https://www.linkedin.com/in/derron-foo-xi-wei-a90896298/)\n",
    "- [Peh Jun Jie Rone](https://www.linkedin.com/in/ronepeh/)\n",
    "- [Gabriel Lim Wen Le](https://www.linkedin.com/in/gabriel-lim-wen-le-3b26612b0/)\n",
    "- [Hoi Sing See](https://www.linkedin.com/in/hoi-sing-see-/)\n",
    "\n",
    "\n",
    "NYP AI's Chat With Your Data Workshop has been proudly delivered to you by the event committee, consisting of:\n",
    "- [Prakhar Nilesh Trivedi](https://linkedin.com/in/prakhartrivedi0706) (OIC, VP)\n",
    "- [Sarah Zoe Sung](https://www.linkedin.com/in/sarah-zoe-sung/) (AIC)\n",
    "- [Derron Foo Xi Wei](https://www.linkedin.com/in/derron-foo-xi-wei-a90896298/) (Materials and Content)\n",
    "- [Peh Jun Jie Rone](https://www.linkedin.com/in/ronepeh/) (Materials and Content)\n",
    "- [Gabriel Lim Wen Le](https://www.linkedin.com/in/gabriel-lim-wen-le-3b26612b0/) (Materials and Content)\n",
    "- [Faith Yeo](https://www.linkedin.com/in/faithyjw/) (Publicity IC)\n",
    "\n",
    "\n",
    "The committee could not have done it without the close collaboration and support of **NYP AI committee members**, and student development executives ***Ms Teo Miow Ting*** and ***Mr Alvin Tay***.\n",
    "\n",
    "\n",
    "---\n",
    "**Inspired to join us** to create value for SIT students across several verticals in AI? [Join us](https://go.nyp.ai/join) or [visit our website](https://nyp.ai).\n",
    "\n",
    "\n",
    "We hope you had an enriching experience and we can't wait to see what you build.\n",
    "\n",
    "<strong>Signing off,<br>\n",
    "NYP Artificial Intelligence<br>\n",
    "NYP School of Information Technlogy</strong>\n",
    "\n",
    "<img src=\"https://about.nyp.ai/static/logo/Dark.png\" alt=\"NYP AI Logo\" height=\"100px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYoAKa4d_RJK"
   },
   "source": [
    "*---- You have reached the end ----*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
