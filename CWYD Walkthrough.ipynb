{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAhsUTqCDeGq"
   },
   "source": [
    "<img src=\"https://about.nyp.ai/static/logo/Dark.png\" alt=\"NYP AI Logo\" height=\"100px\">\n",
    "\n",
    "# **Introduction to LangChain: Chat With Your Data**\n",
    "\n",
    "Updated: 10 July 2024, 10:33PM\n",
    "\n",
    "**Welcome to NYP AI's Chat With Your Data Workshop.**\n",
    "\n",
    "**What?**\n",
    "\n",
    "In this workshop, code along with the instructor and build your own data inference algorithm with the [LangChain Python library](https://langchain.com).\n",
    "\n",
    "**HELP MEE**\n",
    "\n",
    "For Non-Technical or Curious Questions:\n",
    "\n",
    "> Ask them [here](https://qna.nyp.ai/ask) and the instructor will try to answer them in inter-segment breaks or at the Q&A session at the end.\n",
    "\n",
    "For Technical Questions or Having Trouble Following Along:\n",
    "> Feel free to raise your hand at any time and one of the workshop troubleshooters will assist you.\n",
    "\n",
    "***Note: Please be polite and co-operative, we want to ensure you have a good learning experience and we hope that you will allow us to create that.***\n",
    "\n",
    "**How do I start?**\n",
    "\n",
    "For instructions on setting up this notebook, look at the [CWYD Workshop Pre-Requisites](https://docs.google.com/document/d/e/2PACX-1vRwOmZCrFxrWwbTamFt9eBxprybm4_xNUaSUofVW3Iys50IM15i9yF9oqjmWd32GuG6ZCqYMIo3XVFl/pub) document.\n",
    "\n",
    "---\n",
    "\n",
    "We hope you have takeaway valuable skills from today and that you had fun! 🤩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDk6MIQDHzf4"
   },
   "source": [
    "# 0. Setup\n",
    "Let's get started!\n",
    "Start by installing the required libraries and getting your workshop account API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvu8lP0JH8kG"
   },
   "source": [
    "### 0.1 Install required libraries\n",
    "The following libraries and imports are required for this notebook.\n",
    "\n",
    "> **Run** the following cell by clicking on the cell and doing **'Shift + Enter'** or clicking the run button at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ci8E90M6JzSt",
    "outputId": "af17c9f0-ecb3-4545-9dd4-e2d5c85f4da9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: openai in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.35.13)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.1.15)\n",
      "Requirement already satisfied: chromadb in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: lark in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.9)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.16)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.1.85)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.5 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.7.5)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.111.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.18.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.65.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.1.3)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (30.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (3.10.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (24.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (2.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (1.33)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.25.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.46b0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.46b0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (70.3.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.23.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain) (3.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\werty\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.17.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\werty\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All libraries and imports successful!\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai langchain-community langchain-openai chromadb lark\n",
    "import os, sys, json, shutil\n",
    "from pprint import pprint\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "print()\n",
    "print(\"All libraries and imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbnVMBSFLo8O"
   },
   "source": [
    "### 0.2 Get the workshop API Key\n",
    "\n",
    "This API key will be used by LangChain to send structured queries to OpenAI endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "OPEce6J6LuL0"
   },
   "outputs": [],
   "source": [
    "# Run this cell first\n",
    "exec(\"\"\"\\nimport os, sys, json, requests\\n\\ndef injectAPIKey(username,password,injectionKey=\"OPENAI_API_KEY\"):\\n    hd = {\"Content-Type\":\"application/json\",\"APIKey\":\"P@ssw0rd!\"}\\n    d = requests.post(url=\"https://keyserver.replit.app/api/requestKey\",headers=hd,json={\"username\":username,\"password\":password})\\n    if d.text.startswith(\"UERROR\") or d.text.startswith(\"ERROR\"):\\n        raise Exception(\"INJECTAPIKEY ERROR: \" + d.text[len(\"ERROR: \"):])\\n    elif d.text.startswith(\"SUCCESS\"):\\n        os.environ[injectionKey] = d.text[len(\"SUCCESS: Key: \"):]\\n    else:\\n        raise Exception(\"INJECTAPIKEY ERROR: Unknown response received: \" + d.text)\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng2rkwWoljh4"
   },
   "source": [
    "**Where to get the USERNAME and PASSWORD?**\n",
    "\n",
    "Details should've been sent to your email. Pass it into the `injectAPIKey` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ac1bgcOaJGWu"
   },
   "outputs": [],
   "source": [
    "# Uncomment the line of code below and replace parameters with your username and password\n",
    "injectAPIKey(\"231165R\", \"30g1on7w\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3hWK-pVJINl"
   },
   "source": [
    "---\n",
    "\n",
    "# Segment 1 - Loading, Splitting, Embedding\n",
    "Now that you have finished setting up, let's get right into it!\n",
    "\n",
    "Here, you'll be learning the theory of Retrieval Augmented Generation and the different stages in the process.\n",
    "\n",
    "In the hour, we will tough on Document Loaders, Splitters and Embedding splits into a Vector Database; all part of the **Indexing Pipline** shown below.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vAvDBIbr8MnL_Q51mBtBhw.png\" alt=\"Indexing Pipeline\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xydUKvSpJLoS"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What are LLMs?**\n",
    "\n",
    "Large Language Models (LLMs) demonstrate significant capabilities in understanding and generating human language. They can perform various tasks such as answering questions, summarizing text, and generating creative content.\n",
    "\n",
    "However, they **sometimes generate incorrect but believable responses** when they lack information, a **phenomenon known as “hallucination.”** This means they **confidently provide information that may sound accurate but could be incorrect due to outdated or insufficient knowledge**.\n",
    "\n",
    "> In the context of this workshop, LLMs are powerful tools, but they need proper mechanisms to ensure the accuracy and relevance of their responses.\n",
    "\n",
    "## Where RAG comes in...\n",
    "\n",
    "**What is RAG?**\n",
    "\n",
    "Retrieval Augmented Generation (RAG) **addresses the issue of LLM hallucinations** by integrating an information retrieval system into the LLM pipeline. Instead of relying solely on pre-trained knowledge, RAG allows the model to dynamically **fetch information from external knowledge sources when generating responses**. This dynamic retrieval mechanism ensures that the information provided by the LLM is not only **contextually relevant** but also **accurate and up-to-date**.\n",
    "\n",
    "> In summary, RAG enhances the reliability of the conversation by grounding responses in real-time data, making interactions more trustworthy and informative.\n",
    "\n",
    "***Below is a simplified RAG pipeline:***\n",
    "\n",
    "<img src=\"https://assets-global.website-files.com/5ee50f2ef83ac07f0cb7fb44/65847f3073978e597886d087_rag-f517f1f834bdbb94a87765e0edd40ff2.png\" alt=\"RAG Pipeline\" height=\"400px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGg21djaKdXA"
   },
   "source": [
    "## 1.1 Document Loading\n",
    "\n",
    "**What is Document Loading?**\n",
    "\n",
    "The first step is to transform your data into a format conducive for interaction. We do this by embedding your your source data into a semantic numerical format for retrieval, as you'll learn soon.\n",
    "\n",
    "**Why is it Important?**\n",
    "\n",
    "Document loaders play a crucial role in ***accessing and converting data from a multitude of formats and sources into a standardized structure.***\n",
    "\n",
    "We often find ourselves needing to extract data from various origins such as websites, databases, YouTube, and this data can manifest in diverse formats like PDFs, HTML, and JSON. The primary objective of document loaders is to harmonize this data diversity into a unified document object, comprising content and associated metadata.\n",
    "\n",
    "**Where Langchain Comes In...**\n",
    "\n",
    "In LangChain, you'll discover an extensive range of ***document loaders***, roughly categorized into more than 80 distinct types. These loaders cater to unstructured data, such as text files from public sources like YouTube, Twitter, or Hacker News, as well as unstructured data from proprietary sources like Figma or Notion.\n",
    "\n",
    "> Document loaders also extend their capabilities to structured data, often presented in tabular formats, containing text data within cells or rows that are still essential for question answering or semantic search.\n",
    "\n",
    "For this workshop, we will be using a Notion Database of Harry Potter information.\n",
    "\n",
    "> **How to Load Notion Databases (IN GENERAL):**\n",
    ">\n",
    "> 1. Export your Notion space as Markdown/CSV\n",
    "> 2. Enable 'Include subpages' and 'Create folders for subspages'\n",
    "> 3. Unzip the folder and place it in the same folder as this .ipynb file\n",
    "> 4. Use Langchain's Document Loader to load your Notion DB with steps similar to what's shown.\n",
    "\n",
    "Follow the instructions below to setup the Harry Potter Notion DB and load it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhkC5DclJzUB"
   },
   "source": [
    "### Prepare Harry Potter Notion DB\n",
    "\n",
    "> In the same folder as this notebook, [download and unzip the HogwartsDB Notion dump](https://go.nyp.ai/cwydhogwarts).\n",
    "\n",
    "The unzipped folder should directly have 6 files of Harry Potter text; there should be no sub-folders. Some operating systems may auto-create subfolders with the same name in the unzipped folder, so you need to move the files up one folder.\n",
    "\n",
    "Resulting folder structure should look like:\n",
    "```\n",
    "- CWYD Walkthrough.ipynb\n",
    "- HogwartsDB\n",
    "    - Harry Potter and The Sorcerer's Stone.md\n",
    "    - Harry Potter and the Chamber of Secrets.md\n",
    "    - Harry Potter and the Prisoner of Azkaban.md\n",
    "    - Harry Potter and the Goblet of Fire.md\n",
    "    - Harry Potter and the Order of the Phoenix.md\n",
    "    - Harry Potter and the Half-Blood Prince.md\n",
    "```\n",
    "\n",
    "*If you face issues with unzipping and loading the HogwartsDB in the subsequent steps, seek help from a troubleshooter.*\n",
    "\n",
    "> You need to take note of where you're storing this notebook file. If you don't remember, run the cell below to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "P7dHRYl4_uBH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook's Current Folder Path: c:\\Users\\werty\\Downloads\\GitHub\\CWYD-Workshop\n",
      "HogwartsDB folder found in current folder: True\n"
     ]
    }
   ],
   "source": [
    "# (OPTIONAL) RUN TO CHECK CURRENT FOLDER AND TO SEE IF HOGWARTSDB FOLDER IS FOUND\n",
    "print(\"Notebook's Current Folder Path:\", os.getcwd())\n",
    "print(\"HogwartsDB folder found in current folder:\", os.path.isdir(os.path.join(os.getcwd(), \"HogwartsDB\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEZeHeYa_uBH"
   },
   "source": [
    "### Loading HogwartsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "c2OT5kMKJ3yz"
   },
   "outputs": [],
   "source": [
    "# Initialise a NotionDirectoryLoader and load the database\n",
    "loader = NotionDirectoryLoader(\"HogwartsDB\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "qMIVqHM5NJOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and the Chamber of Secrets.md'}, page_content='# Harry Potter and the Chamber of Secrets\\n\\n## Premise\\n\\nIn \"Harry Potter and the Chamber of Secrets,\" the second book in the Harry Potter series by J.K. Rowling, the story follows Harry Potter\\'s second year at Hogwarts School of Witchcraft and Wizardry. The premise revolves around the mystery of the Chamber of Secrets, a hidden chamber within Hogwarts believed to have been created by one of the school\\'s founders, Salazar Slytherin. Legend has it that the Chamber can only be opened by Slytherin\\'s heir and that it houses a deadly creature.\\n\\nWhen students at Hogwarts start turning up petrified, suspicion falls on Harry as the culprit, especially since he can speak Parseltongue, the language of snakes, which is associated with Slytherin. Together with his friends Ron and Hermione, Harry sets out to uncover the truth behind the Chamber of Secrets and clear his name. Along the way, they encounter dangerous challenges, encounter new magical creatures, and ultimately confront the true culprit behind the attacks. The book explores themes of prejudice, loyalty, and bravery, while delving deeper into the magical world of Harry Potter.\\n\\n## New Characters\\n\\n1. **Gilderoy Lockhart**: The new Defense Against the Dark Arts teacher at Hogwarts, known for his charm, flamboyance, and supposed exploits in defeating dark creatures. He quickly becomes a favorite among the students, though his true abilities are questionable.\\n2. **Dobby**: A house-elf who warns Harry not to return to Hogwarts for his second year, fearing for his safety. Dobby is initially presented as eccentric and overly dramatic, but his true motivations and loyalty become clearer as the story progresses.\\n3. **Ginny Weasley**: Ron\\'s younger sister, who starts her first year at Hogwarts in this book. Ginny plays a significant role in the story, as she becomes possessed by the diary of Tom Riddle and inadvertently opens the Chamber of Secrets.\\n4. **Colin Creevey**: A Muggle-born student who idolizes Harry and constantly takes pictures of him with his magical camera. He is one of the first victims of the attacks in the book.\\n5. **Moaning Myrtle**: The ghost of a student who haunts the girls\\' bathroom on the second floor of Hogwarts. She becomes involved in the mystery surrounding the Chamber of Secrets after she is killed by the basilisk.\\n\\n## Plot\\n\\nThe plot of \"Harry Potter and the Chamber of Secrets\" revolves around Harry Potter\\'s second year at Hogwarts School of Witchcraft and Wizardry and the mystery surrounding the Chamber of Secrets.\\n\\nAt the beginning of the school year, Harry begins to receive ominous warnings that something terrible will happen if he returns to Hogwarts. Despite these warnings, Harry returns to school and discovers that strange things are indeed happening. Students are being mysteriously petrified, and ominous messages written in blood appear on the walls, proclaiming that the Chamber of Secrets has been opened and that the heir of Slytherin will unleash terror upon the school.\\n\\nAs the attacks escalate, suspicion falls on Harry, particularly because of his ability to speak Parseltongue, the language of snakes. Harry, along with his friends Ron and Hermione, sets out to uncover the truth behind the Chamber of Secrets and clear his name. They delve into the history of Hogwarts, learning about the legend of the Chamber and the dark history of Salazar Slytherin.\\n\\nThroughout their investigation, Harry and his friends encounter various obstacles and challenges, including encounters with magical creatures, hostile teachers, and dangerous secrets hidden within the castle. Eventually, they uncover the entrance to the Chamber of Secrets and confront the true culprit behind the attacks: Tom Riddle, a memory preserved in a diary that possesses Ginny Weasley. Riddle reveals himself to be a younger version of Voldemort, the dark wizard who tried to kill Harry when he was a baby.\\n\\nWith the help of Dumbledore\\'s phoenix, Fawkes, Harry defeats the basilisk that had been terrorizing the school and destroys the diary, thus ending the threat of the Chamber of Secrets. The school celebrates Harry and his friends as heroes, and Harry learns more about his own connection to Voldemort and the dark forces that threaten the wizarding world.\\n\\n## Reader’s Take\\n\\nMany have found \"Harry Potter and the Chamber of Secrets\" to be a thrilling continuation of Harry\\'s adventures at Hogwarts. The book maintains the magical atmosphere and charm of the wizarding world while delving deeper into its darker aspects. The mystery surrounding the Chamber of Secrets keeps me engaged as I try to unravel the clues alongside Harry and his friends.\\n\\nMany appreciate the introduction of new characters, such as Gilderoy Lockhart and Dobby, who add humor and depth to the story. Ginny Weasley\\'s role in the plot is also significant, and her character development throughout the book is often impressive.\\n\\nThe themes of loyalty, friendship, and bravery resonate with me, as Harry and his friends demonstrate courage in the face of danger and adversity. The exploration of prejudice against Muggle-borns and the consequences of bigotry adds depth to the story and prompts me to reflect on real-world issues.\\n\\nOverall, \"Harry Potter and the Chamber of Secrets\" is often a captivating and exciting installment in the series, setting the stage for even greater adventures to come.')\n"
     ]
    }
   ],
   "source": [
    "# Check if data has loaded\n",
    "pprint(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Ma3i2r5PO-Oh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'HogwartsDB\\\\Harry Potter and the Chamber of Secrets.md'}\n"
     ]
    }
   ],
   "source": [
    "# See the metadata of the database\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neU74-QaWy_R"
   },
   "source": [
    "## 1.2 Text / Document Splitting\n",
    "\n",
    "**What is Document Splitting?**\n",
    "\n",
    "Document splitting is a pre-processing step that ***divides large textual documents into smaller segments or chunks***. This technique is essential for ***managing and processing large volumes of text efficiently***, especially in natural language processing (NLP) tasks.\n",
    "\n",
    "Once documents are split, each segment or chunk becomes more manageable for further analysis and processing. This segmentation allows NLP models to handle pieces of text individually, improving computational efficiency and enabling more targeted analysis.\n",
    "\n",
    "**Chunking Method using Fixed Chunk Sizes & Overlapping**\n",
    "\n",
    "One method to document splitting is by...\n",
    "\n",
    "**Chunk Size**\n",
    "\n",
    "The size of the chunked data is going to make a huge difference in what information comes up in a search. When you embed a piece of data, the whole thing is converted into a vector. Include too much in a chunk and the vector loses the ability to be specific to anything it discusses. Include too little and you lose the context of the data.\n",
    "\n",
    "> In short, size matters. 😁\n",
    "\n",
    "**Chunk Overlapping**\n",
    "\n",
    "For some LangChain splitters, you can specify a specific chunk overlap; chunk overlaps help to precede chunks with information from the previous chunk so that the chunk split is not too abrupt. The specified quantity of overlap is included in both the end and the beginning of the first and second chunks respectively.\n",
    "\n",
    "This helps chunks be more useful and not too abrupt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfQa_j7nXC-k"
   },
   "source": [
    "### Common LangChain Text Splitters\n",
    "\n",
    "LangChain provides an extensive range of different text splitters. Some common ones include:\n",
    "- Character Text Splitter\n",
    "- Token Text Splitter\n",
    "- Recursive Character Text Splitter\n",
    "- Markdown Header Text Splitter (also known as 'Context-aware chunking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc0f8Bn7_uBI"
   },
   "source": [
    "### Understanding LangChain Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "jaKR-fDZ_uBI"
   },
   "outputs": [],
   "source": [
    "# Initialise a CharacterTextSplitter and RecursiveCharacterTextSplitter\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=26,\n",
    "    chunk_overlap=4\n",
    ")\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=26,\n",
    "    chunk_overlap=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Ep99tN1d_uBI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split text1 with r_splitter\n",
    "text1 = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "NtPPO_D7_uBI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefghijklmnopqrstuv', 'stuvwxyz']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split text2 with r_splitter\n",
    "text2 = \"abcdefghijklmnopqrstuvwxyz\" * 2\n",
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "kBBVk0bH_uBI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split text3 with r_splitter\n",
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "r_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "uJA28bHN_uBI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m n o p q r s t u v w x y z']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split text3 with c_splitter\n",
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "fIkyFCQs_uBI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split text3 with new c_splitter with space separator\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=26,\n",
    "    chunk_overlap=4,\n",
    "    separator=\" \"\n",
    ")\n",
    "\n",
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7QT1sUsSJAj"
   },
   "source": [
    "### Splitting HogwartsDB (RecursiveSplitter)\n",
    "\n",
    "For this workshop, we will be using `RecursiveCharacterTextSplitter` to split our data. As you'll learn, the splitter splits based on a list of separators, ordered by priority in terms of highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "LV0aSISjW3fO"
   },
   "outputs": [],
   "source": [
    "# Define reasonable chunk parameters\n",
    "chunk_size = 350\n",
    "chunk_overlap = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "5U-E7QAA_uBI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    }
   ],
   "source": [
    "# Initialise new RecursiveCharacterTextSplitter and split with split_documents\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=/.)\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "splits = r_splitter.split_documents(docs)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr9IpGVb_uBI"
   },
   "source": [
    "## 1.3 Embedding Chunks into a Vector Store / Database\n",
    "\n",
    "**What are Embeddings?**\n",
    "\n",
    "Embeddings are numerical representations of real-world objects that machine learning (ML) and AI systems use to understand complex knowledge domains like humans do.\n",
    "\n",
    "As an example, computing algorithms understand that the difference between 2 and 3 is 1, indicating a close relationship between 2 and 3 as compared to 2 and 100.\n",
    "\n",
    "We will be using the `OpenAIEmbeddings` module, which uses embedding models made by OpenAI.\n",
    "\n",
    "**What are Vectorstores?**\n",
    "\n",
    "A vector store is an actual system or platform to handle the complexities and specifics of vector data, like embeddings, often in association with a vector database. They are very commonly used in AI and ML applications.\n",
    "\n",
    "Popular examples of vector databases include Pinecone, Chroma and many more.\n",
    "\n",
    "**Importance of Vectorstores for Embeddings**\n",
    "\n",
    "By storing embeddings in a vector store, we can perform really efficient searches and retrievals, allowing us to retrieve the most relevant documents or chunks of text for a given query.\n",
    "\n",
    "There are many vectorstores that you can use to store your embeddings. For this workshop, we will be using ChromaDB to store our Hogwarts Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTy1mirK_uBI"
   },
   "source": [
    "### Embedding our DB Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "HrvNwQhT_uBJ"
   },
   "outputs": [],
   "source": [
    "# Initialise OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "FNv0fXTI_uBJ"
   },
   "outputs": [],
   "source": [
    "# Initialize a Chroma vector database. Persist in a './db/chroma' folder.\n",
    "persist_directory = \"./db/chroma\"\n",
    "\n",
    "vectorDB = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "w-pEy368_uBJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    }
   ],
   "source": [
    "# Check vectorDB collection count\n",
    "print(vectorDB._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLrN1P2t_uBJ"
   },
   "source": [
    "## 1.4 Segment 1 Checkpoint\n",
    "\n",
    "Wow, that was intense! Let's **summarise** what we have learnt so far:\n",
    "1. **Document Loading**\n",
    "  - Using a few of LangChain's loaders to load a Notion Dump\n",
    "\n",
    "2. **Document Splitting**\n",
    "  - Using LangChain's different splitters to split different kinds of data in different ways\n",
    "\n",
    "3. **Embeddings**\n",
    "  - Using the `OpenAIEmbeddings` module (and the `Text-embedding-ada-002-v2` model) to embed splits\n",
    "\n",
    "4. **Vector Storing**\n",
    "  - Storing embeddings into a local vector `Chroma` database\n",
    "\n",
    "\n",
    "While we have only went through the basics, we do encourge you guys to **stay curious** and explore more on the different methods for each step!\n",
    "\n",
    "Explore:\n",
    "- [All the different LangChain loaders available](https://python.langchain.com/v0.2/docs/integrations/document_loaders/)\n",
    "- [Explore different data splitters and parameters](https://python.langchain.com/v0.2/docs/integrations/document_transformers/)\n",
    "- [Learn about how embedding models work](https://medium.com/@eugenesh4work/what-are-embeddings-and-how-do-it-work-b35af573b59e)\n",
    "- [Learn about different Vector stores](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)\n",
    "- [Learn more about Chroma](https://www.trychroma.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dbOU9qH_uBJ"
   },
   "source": [
    "---\n",
    "# Segment 2 - Retrieval Algorithms\n",
    "\n",
    "**What is Retrieval?**\n",
    "\n",
    "After storing our embeddings into a vector store, we can must now look at how we can retrieve the appropriate splits that is relevant to our Prompt / Search Query to load into the LLM.\n",
    "\n",
    "**Importance of Retrieval Algorithms**\n",
    "\n",
    "Retrieval algorithms are then important since they are the core techniques for the retrieval of data in response to a user's query. They are responsible for retrieving information that is potentially useful for the LLM to answer the user appropriately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0KfLMjq_uBK"
   },
   "source": [
    "## 2.1 Common Retrieval Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8TONHnH_uBK"
   },
   "source": [
    "### Semantic Similarity Search\n",
    "\n",
    "**How it works?**\n",
    "\n",
    "Taking advantage of a vector database's properties, this technique allows you to retrieve the most similar document chunks for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "pm-ed18k_uBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and The Sorcerers Stone.md'}, page_content='- **Hermione Granger:** Hermione’s character develops significantly over the course of the story and sheds light on Harry’s character as well. At the outset, she is an annoying perfectionist, a goody-two-shoes who has read all the books for her classes in advance, has learned all about Hogwarts, and never breaks the rules. When she first speaks to'),\n",
      " Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and The SorcererΓÇÖs Stone.md'}, page_content='- **Hermione Granger:** Hermione’s character develops significantly over the course of the story and sheds light on Harry’s character as well. At the outset, she is an annoying perfectionist, a goody-two-shoes who has read all the books for her classes in advance, has learned all about Hogwarts, and never breaks the rules. When she first speaks to'),\n",
      " Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and the Chamber of Secrets.md'}, page_content=\"3. **Ginny Weasley**: Ron's younger sister, who starts her first year at Hogwarts in this book. Ginny plays a significant role in the story, as she becomes possessed by the diary of Tom Riddle and inadvertently opens the Chamber of Secrets.\")]\n"
     ]
    }
   ],
   "source": [
    "# Carry out a basic semantic similarity search\n",
    "userQuery = input(\"Enter a query: \")\n",
    "chunks = vectorDB.similarity_search(userQuery, k=3)\n",
    "pprint(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaV0UdSc_uBK"
   },
   "source": [
    "### Filtered Similarity Search\n",
    "\n",
    "Building on the basic semantic similarity search, we can add in a filter to it.\n",
    "This `filter` parameter limits the search to ONLY retrieve from the splits inside the stated document souce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "_T3Vce0Y_uBK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\H'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\H'\n",
      "C:\\Users\\werty\\AppData\\Local\\Temp\\ipykernel_7124\\235750175.py:6: SyntaxWarning: invalid escape sequence '\\H'\n",
      "  filter={\"source\": \"HogwartsDB\\Harry Potter and the Chamber of Secrets.md\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and the Chamber of Secrets.md'}, page_content=\"3. **Ginny Weasley**: Ron's younger sister, who starts her first year at Hogwarts in this book. Ginny plays a significant role in the story, as she becomes possessed by the diary of Tom Riddle and inadvertently opens the Chamber of Secrets.\"),\n",
      " Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and the Chamber of Secrets.md'}, page_content=\"3. **Ginny Weasley**: Ron's younger sister, who starts her first year at Hogwarts in this book. Ginny plays a significant role in the story, as she becomes possessed by the diary of Tom Riddle and inadvertently opens the Chamber of Secrets.\"),\n",
      " Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and the Chamber of Secrets.md'}, page_content=\"5. **Moaning Myrtle**: The ghost of a student who haunts the girls' bathroom on the second floor of Hogwarts. She becomes involved in the mystery surrounding the Chamber of Secrets after she is killed by the basilisk.\")]\n"
     ]
    }
   ],
   "source": [
    "# Filtered Similarity Search\n",
    "userQuery = input(\"Enter a query: \")\n",
    "chunks =vectorDB.similarity_search(\n",
    "    userQuery,\n",
    "    k=3,\n",
    "    filter={\"source\": \"HogwartsDB\\Harry Potter and the Chamber of Secrets.md\"}\n",
    ")\n",
    "pprint(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMn5abFG_uBK"
   },
   "source": [
    "### MMR Search (Diverse retrieval)\n",
    "\n",
    "**How it works?**\n",
    "\n",
    "The idea behind Maximum Marginal Relevance (MMR) is to reduce redundancy and increase diversity in the results. MMR selects the phrase in the final keyphrases list according to a combined criterion of query relevance and novelty of information.\n",
    "\n",
    "In LangChain, you provide a initial `fetch_k` to indicate the number of similar chunks you want to retrieve. From this, the specified `k` **diverse** chunks will be returned as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "cbX_dvMi_uBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and The Sorcerers Stone.md'}, page_content='- **Hermione Granger:** Hermione’s character develops significantly over the course of the story and sheds light on Harry’s character as well. At the outset, she is an annoying perfectionist, a goody-two-shoes who has read all the books for her classes in advance, has learned all about Hogwarts, and never breaks the rules. When she first speaks to'),\n",
      " Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and the Chamber of Secrets.md'}, page_content=\"3. **Ginny Weasley**: Ron's younger sister, who starts her first year at Hogwarts in this book. Ginny plays a significant role in the story, as she becomes possessed by the diary of Tom Riddle and inadvertently opens the Chamber of Secrets.\")]\n"
     ]
    }
   ],
   "source": [
    "# Maximum Marginal Relevance Search (Diverse retrieval)\n",
    "userQuery = input(\"Enter a query: \")\n",
    "chunks = vectorDB.max_marginal_relevance_search(\n",
    "    userQuery,\n",
    "    k=2,\n",
    "    fetch_k=3\n",
    ")\n",
    "pprint(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZX9WcKa_uBK"
   },
   "source": [
    "## 2.2 BONUS: Self-query Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAuB0q5H_uBL"
   },
   "source": [
    "Often, you want to infer from the metadata itself.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes.\n",
    "\n",
    "[Try out self-query retrieval by referring to this.](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/self_query/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWLbBcP9_uBL"
   },
   "source": [
    "## 2.3 Segment 2 Checkpoint\n",
    "\n",
    "This is a good point to stop and explore for a second. Take a review of all that you've learned in this section. Try out different kinds of queries and see the outputs you get. Play around with the parameters you pass in and see what parameters work the best.\n",
    "\n",
    "\n",
    "You can also explore other kinds of search like `asimilarity_search`, `similarity_search_with_score` and many more. Try passing different parameters to the retrieval chains and experimenting with different prompts.\n",
    "\n",
    "[Learn more about the large variety and complexities of LangChain retrievers here](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDomJTTm_uBL"
   },
   "source": [
    "---\n",
    "# Segment 3 - Question Answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOS5QdP9_uBL"
   },
   "source": [
    "## 3.0 Setting Up LangSmith (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8l7x9aE_uBL"
   },
   "source": [
    "This is entirely optional; the instructor will show you the LangSmith console during the workshop to explain what's going on.\n",
    "\n",
    "The benefit of linking up to the LangSmith platform is the ability to visualise the LLM calls and different steps a chain takes.\n",
    "\n",
    "If you want to link up with LangSmith, carry out the following:\n",
    "- Go to [LangSmith](https://www.langchain.com/langsmith) and sign up\n",
    "- Create an API key from your account settings\n",
    "- Uncomment the code below and use your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTYuS0sj_uBL"
   },
   "outputs": [],
   "source": [
    "# Set up LangSmith\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"...\" # replace dots with your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2kh7lBl7nE7"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**What is Retrieval Questions Answering Chains?**\n",
    "\n",
    "Retrieval QA chains are designed for question-answeing tasks where the answer is retrieved from a given context. Chains are highly modular; you can combine them with other chains, re-order them and even introduce your own steps in between.\n",
    "\n",
    "**Importance of 'Chains'**\n",
    "\n",
    "Retrieval chains play an important role in the retrieval process, providing a streamlined process of flow and maintaining the efficiency and relevancy of information extracted from external sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O47i0s2x_uBL"
   },
   "source": [
    "## 3.1 Stuff QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAU7IvZY_uBL"
   },
   "source": [
    "### Making Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "8FjpCdrD_uBL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\werty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialise a PromptTemplate with a given string template\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # try experimenting temperature with values from 0-1\n",
    "\n",
    "# Build your prompt\n",
    "template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, you can say \"I don't know\", don't try to make up an answer. Use three sentences maximum. Say \"Thanks for asking!\" at the end of your answer.\n",
    "{context}\n",
    "Question: {question} \n",
    "Helpful answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYV13umD_uBL"
   },
   "source": [
    "### Running a QA Chain (Stuff Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "2kCMgDz9_uBL"
   },
   "outputs": [],
   "source": [
    "# Initialise a RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorDB.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "LMb4V2G8_uBM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\werty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hermione Granger is a character in the Harry Potter series who starts off as a perfectionist and rule-follower but grows and changes throughout the story. She is known for her intelligence, loyalty, and bravery in helping Harry and Ron on their adventures. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "# Run user query through the chain\n",
    "userQuery = input(\"Enter a query: \")\n",
    "result = qa_chain({\"query\": userQuery})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "W-SYJrPi_uBM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and The Sorcerers Stone.md'}, page_content='- **Hermione Granger:** Hermione’s character develops significantly over the course of the story and sheds light on Harry’s character as well. At the outset, she is an annoying perfectionist, a goody-two-shoes who has read all the books for her classes in advance, has learned all about Hogwarts, and never breaks the rules. When she first speaks to'),\n",
      " Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and The SorcererΓÇÖs Stone.md'}, page_content='- **Hermione Granger:** Hermione’s character develops significantly over the course of the story and sheds light on Harry’s character as well. At the outset, she is an annoying perfectionist, a goody-two-shoes who has read all the books for her classes in advance, has learned all about Hogwarts, and never breaks the rules. When she first speaks to'),\n",
      " Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and the Chamber of Secrets.md'}, page_content=\"3. **Ginny Weasley**: Ron's younger sister, who starts her first year at Hogwarts in this book. Ginny plays a significant role in the story, as she becomes possessed by the diary of Tom Riddle and inadvertently opens the Chamber of Secrets.\"),\n",
      " Document(metadata={'source': 'HogwartsDB\\\\Harry Potter and the Chamber of Secrets.md'}, page_content=\"3. **Ginny Weasley**: Ron's younger sister, who starts her first year at Hogwarts in this book. Ginny plays a significant role in the story, as she becomes possessed by the diary of Tom Riddle and inadvertently opens the Chamber of Secrets.\")]\n"
     ]
    }
   ],
   "source": [
    "# See the result's source documents\n",
    "pprint(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6XAm0IP_uBM"
   },
   "source": [
    "The stuff technique is really good because it involves only one call to the language model.\n",
    "\n",
    "The problem with this is that if there's too many documents, they may not all be able to fit in the LLM's context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUEkKyVe_uBM"
   },
   "source": [
    "## 3.2 MapReduce QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRe9Yx5s_uBM"
   },
   "source": [
    "In the Map Reduce technique, each retrieved chunk is passed into individual LLM calls to be summarised.\n",
    "\n",
    "These summarised chunks are then stuffed into one final LLM call with the user's prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuusGKF6_uBM"
   },
   "source": [
    "### Create a MapReduce chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "A57qsRrF_uBM"
   },
   "outputs": [],
   "source": [
    "# Initialise a MapReduce RetrievalQA\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorDB.as_retriever(),\n",
    "    chain_type=\"map_reduce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "1R3cK3OU_uBM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salazar is a reference to Salazar Slytherin, one of the four founders of Hogwarts School of Witchcraft and Wizardry in the Harry Potter series. He was known for his cunning and ambition, as well as his belief in the importance of pure-blood wizarding families.\n"
     ]
    }
   ],
   "source": [
    "# Run user query through the MapReduce chain\n",
    "userQuery = input(\"Enter a query: \")\n",
    "result = qa_chain_mr({\"query\": userQuery})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDCB2-23_uBM"
   },
   "source": [
    "> Note how the map reduce chain took **significantly longer**? In some cases, map reduce even **performs worse than a stuff technique**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-ZyhbTf_uBM"
   },
   "source": [
    "### Why is it taking longer?\n",
    "\n",
    "This is due to a few reasons that you can uncover by looking at the run trace in LangSmith:\n",
    "- MapReduce summarises each retrieved chunk in separate LLM calls first\n",
    "- These summarised chunks are then stuffed into a regular `StuffDocumentsChain` with a call to the LLM with the initial user query.\n",
    "- However, **these summarised chunks may not be an accurate representation or may have missing information from the original chunk**, explaining the longer wait times and the inaccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_TMAn7O_uBM"
   },
   "source": [
    "## 3.3 Refine QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIjVbWY0_uBM"
   },
   "source": [
    "In a chain using the refine technique, LangChain will invoke sequential calls to the LLM.\n",
    "\n",
    "In each call, LangChain provides a chunk or more of context to the LLM and prompts with the user question. In subsequent calls, the previous response is **combined with new data/chunks and the LLM is prompted to refine it's original answer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Bt-6-B4_uBM"
   },
   "source": [
    "### Create a Refine chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "KbspemzD_uBM"
   },
   "outputs": [],
   "source": [
    "# Initialise a Refine RetrievalQA\n",
    "qa_chain_refine = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorDB.as_retriever(),\n",
    "    chain_type=\"refine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ZCyxjGZH_uBN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salazar is a reference to Salazar Slytherin, one of the four founders of Hogwarts School of Witchcraft and Wizardry in the Harry Potter series by J.K. Rowling. He was known for his cunning and ambition, as well as his belief in the superiority of pure-blood wizards. Salazar Slytherin also had a dark history, as he was known for his views on blood purity and his creation of the Chamber of Secrets, which housed a deadly creature that could only be opened by Slytherin's heir. This chamber was created as a means to purge the school of Muggle-born students, causing fear and tension within the Hogwarts community. Legend has it that the Chamber can only be opened by Slytherin's heir and that it houses a deadly creature.\n"
     ]
    }
   ],
   "source": [
    "# Run user query through the Refine chain\n",
    "userQuery = input(\"Enter a query: \")\n",
    "result = qa_chain_refine({\"query\": userQuery})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxqcwLPl_uBN"
   },
   "source": [
    "As you can see, through iterative refinements, the LLM's output is much more well-phrased and comprehensive.\n",
    "\n",
    "The output is also better than when you ran the map reduce chain, because the refine chain actually emphasises more carrying over of information than the former chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KRMOStX_uBN"
   },
   "source": [
    "But, you still can't ask follow up questions. The whole point of a chatbot is to be able to have follow-up questions right?\n",
    "\n",
    "**Let's fix that.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bv3gK6v_uBN"
   },
   "source": [
    "## 3.4 Conversational Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM1SoaOO_uBN"
   },
   "source": [
    "INTRODUCE A BIT OF THEORY ABOUT CONVERSATIONS, AND HOW THEY USE MEMORY BUFFERS. INTRODUCE LANGCHAIN'S CONVERSATIONALRETRIEVALCHAIN.\n",
    "\n",
    "**What are RAG Conversations?**\n",
    "\n",
    "Thanks to the modular architecture of chains, you can nest chains within each other and pass data to and fro.\n",
    "\n",
    "As a result, you can then create conversational chains, so that, while simultaneously retrieving the most relevant information, information about the conversation history is also included to make the answer well-informed.\n",
    "\n",
    "**What are Memory Buffers?**\n",
    "\n",
    "Memory buffers in LangChain allow for the storing of messages which are later formatted into input variables for the prompt. This tool allows you to quickly maintain conversation state and create powerful conversational chains.\n",
    "\n",
    "**What is a `ConversationalRetrievalChain`?**\n",
    "\n",
    "`ConversationalRetrievalChain` is a module from LangChain which allows you to quick create a conversational interface with your data, provided a memory buffer and vector database to retrieve from. This is how we will create a conversational chat interface to talk to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVr26Q8e_uBN"
   },
   "source": [
    "### Create a Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVPWSVpa_uBN"
   },
   "outputs": [],
   "source": [
    "# Initialise a ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XclV0mSd_uBN"
   },
   "source": [
    "### Create a ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSuK82uZ_uBN"
   },
   "outputs": [],
   "source": [
    "# Initialise a ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNsBERHU_uBN"
   },
   "outputs": [],
   "source": [
    "# Run sequential user queries through the chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwrlhUq9_uBN"
   },
   "source": [
    "***And that's it!***\n",
    "\n",
    "**Congratulations! You can now *Chat With Your Data!*** 🤯🎉🥳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNSEXTxD_uBN"
   },
   "source": [
    "## 3.5 Segment 3 Checkpoint\n",
    "\n",
    "Excitingly, now you have finally created a working chat interface with your own custom data.\n",
    "\n",
    "Now that you've written the algorithm, hopefully you can see how it all falls into place together to **create a streamlined Retrieval Augmented Generation workflow.**\n",
    "\n",
    "This workflow algorithm is highly modular, you can substitute, modify, add, remove any components or logic however you want, as long as the core concepts and procedures of RAG are there. You can introduce your own custom logic as well for more niche use cases.\n",
    "\n",
    "**As a recap of this segment, you:**\n",
    "- Created a stuff `RetrievalQA`, where you discovered that it may not be ideal for cases where the documents overflow the LLM's context window\n",
    "- Created a map reduce `RetrievalQA`, which summarises chunks (\"reduces\") and then collates them into one final LLM call. But, map reduce is often inaccurate\n",
    "- Created a refine `RetrievalQA`, which incrementally refines an LLM's outputs by combining new data/chunks with previous answers to the prompt\n",
    "\n",
    "\n",
    "**Cheatsheet:**\n",
    "- Fastest - Stuff QA\n",
    "- Slowest - MapReduce QA, Refine QA\n",
    "- Most Accurate & Comprehensive - Refine QA\n",
    "- Least LLM calls - Stuff QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKFF1BEG_uBN"
   },
   "source": [
    "---\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47twLLj1_uBO"
   },
   "source": [
    "**Give yourself a pat on the back for successfully following through this workshop and creating your own data-inferring chatbot! This is no small feat! 🎉**\n",
    "\n",
    "\n",
    "**Let's recap all that you have learnt:**\n",
    "- Loading structured and unstructured data with LangChain loaders, especially `NotionDirectoryLoader`\n",
    "- Splitting data in different ways with `RecursiveCharacterTextSplitter` and `CharacterTextSplitter`\n",
    "- Embedding data with `OpenAIEmbeddings` in a local `chroma` vector database\n",
    "- Implementing basic retrieval algorithms like `similarity_search` and `max_marginal_relevance`\n",
    "- Answering questions with chains using `stuff`, `map_reduce` and `refine` techniques\n",
    "- Creating a `ConversationalRetrievalChain` where you can ask follow-up prompts\n",
    "\n",
    "\n",
    "**So, What's next?**\n",
    "\n",
    "You've just learnt the basics of Retrieval Augmented Generation with LangChain in Python. You are now fully equipped to integrate these RAG algorithms into your own personal/school projects for an amazing new AI-powered touchpoint with your users.\n",
    "\n",
    "Additionally, empowered by the basic knowledge, you can go on to further research RAG and all the complex upgrades you introduce in your own algorithms. The world is full of possibilities; **go crazy!**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **CREDITS**\n",
    "\n",
    "**Proudly delivered by...**\n",
    "\n",
    "> This workshop was a massive undertaking in the works for **more than three months** of efforts various people in the **NYP AI Student Interest Group**.\n",
    "\n",
    "\n",
    "At NYP AI internally, we aimed to train ourselves in RAG and developed our own project with team members consisting of:\n",
    "- [Prakhar Nilesh Trivedi](https://linkedin.com/in/prakhartrivedi0706)\n",
    "- [Sarah Zoe Sung](https://www.linkedin.com/in/sarah-zoe-sung/)\n",
    "- [Derron Foo Xi Wei](https://www.linkedin.com/in/derron-foo-xi-wei-a90896298/)\n",
    "- [Peh Jun Jie Rone](https://www.linkedin.com/in/ronepeh/)\n",
    "- [Gabriel Lim Wen Le](https://www.linkedin.com/in/gabriel-lim-wen-le-3b26612b0/)\n",
    "- [Hoi Sing See](https://www.linkedin.com/in/hoi-sing-see-/)\n",
    "\n",
    "\n",
    "NYP AI's Chat With Your Data Workshop has been proudly delivered to you by the event committee, consisting of:\n",
    "- [Prakhar Nilesh Trivedi](https://linkedin.com/in/prakhartrivedi0706) (OIC, VP)\n",
    "- [Sarah Zoe Sung](https://www.linkedin.com/in/sarah-zoe-sung/) (AIC)\n",
    "- [Derron Foo Xi Wei](https://www.linkedin.com/in/derron-foo-xi-wei-a90896298/) (Materials and Content)\n",
    "- [Peh Jun Jie Rone](https://www.linkedin.com/in/ronepeh/) (Materials and Content)\n",
    "- [Gabriel Lim Wen Le](https://www.linkedin.com/in/gabriel-lim-wen-le-3b26612b0/) (Materials and Content)\n",
    "- [Faith Yeo](https://www.linkedin.com/in/faithyjw/) (Publicity IC)\n",
    "\n",
    "\n",
    "The committee could not have done it without the close collaboration and support of **NYP AI committee members**, and student development executives ***Ms Teo Miow Ting*** and ***Mr Alvin Tay***.\n",
    "\n",
    "\n",
    "---\n",
    "**Inspired to join us** to create value for SIT students across several verticals in AI? [Join us](https://go.nyp.ai/join) or [visit our website](https://nyp.ai).\n",
    "\n",
    "\n",
    "We hope you had an enriching experience and we can't wait to see what you build.\n",
    "\n",
    "<strong>Signing off,<br>\n",
    "NYP Artificial Intelligence<br>\n",
    "NYP School of Information Technlogy</strong>\n",
    "\n",
    "<img src=\"https://about.nyp.ai/static/logo/Dark.png\" alt=\"NYP AI Logo\" height=\"100px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYoAKa4d_RJK"
   },
   "source": [
    "*---- You have reached the end ----*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
